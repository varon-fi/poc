{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxxWBZioi0N"
      },
      "source": [
        "# Solve Trading Bot using Rainbow DQN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJIVLLT1nYMl"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install numpy pandas pytorch-lightning tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "print(f'device {device} num_gpus {num_gpus}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLH52SgC0RRI"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZO9u9X1CTtf"
      },
      "outputs": [],
      "source": [
        "import math \n",
        "from torch.nn.init import kaiming_uniform_, zeros_\n",
        "\n",
        "# Noisey DQN\n",
        "# Add w_sigma and b_sigma components to the linear equation to add random noise\n",
        "# These components help determine the significance of individual paramters efficiently\n",
        "# Eliminates the need for \"epsilon greedy\" policy\n",
        "class NoisyLinear(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, out_features, sigma):\n",
        "    super(NoisyLinear, self).__init__()\n",
        "    self.w_mu = nn.Parameter(torch.empty((out_features, in_features)))\n",
        "    self.w_sigma = nn.Parameter(torch.empty((out_features, in_features)))\n",
        "    self.b_mu = nn.Parameter(torch.empty((out_features)))\n",
        "    self.b_sigma = nn.Parameter(torch.empty((out_features)))\n",
        "\n",
        "    kaiming_uniform_(self.w_mu, a=math.sqrt(5))\n",
        "    kaiming_uniform_(self.w_sigma, a=math.sqrt(5))\n",
        "    zeros_(self.b_mu)\n",
        "    zeros_(self.b_sigma)\n",
        "    \n",
        "\n",
        "    self.sigma = sigma\n",
        "  def forward(self, x):\n",
        "    if self.training:\n",
        "      w_noise = torch.normal(0, self.sigma, size=self.w_mu.size()).to(device)\n",
        "      b_noise = torch.normal(0, self.sigma, size=self.b_mu.size()).to(device)\n",
        "      return F.linear(x, self.w_mu + self.w_sigma * w_noise, self.b_mu + self.b_sigma * b_noise)\n",
        "    else:\n",
        "      return F.linear(x, self.w_mu, self.b_mu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6gm8-15nYq7"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_shape, n_actions, atoms=51, sigma=0.5):\n",
        "    \"\"\" Deep Q-Network NN Module\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): The # of features in our hidden layers\n",
        "        obs_shape (int): The # of features in our state tensor\n",
        "        n_actions (int): The # of q values our network produces\n",
        "        atoms (int, optional): The # of atomic segments in each q value distribution. Defaults to 51.\n",
        "        sigma (float, optional): Represents the std dev of the noise in each parameter. Defaults to 0.5.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Distributional DQN\n",
        "    # We break our output layers into \"atoms\" and output a distrbution across a range v_min - v_vmax\n",
        "    self.atoms = atoms\n",
        "    self.n_actions = n_actions\n",
        "    \n",
        "    self.net = nn.Sequential(\n",
        "      # Noisey DQN\n",
        "      # Add w_sigma and b_sigma components to the linear equation to add random noise\n",
        "      # These components help determine the significance of individual paramters efficiently\n",
        "      # Eliminates the need for \"epsilon greedy\" policy\n",
        "      NoisyLinear(obs_shape, hidden_size, sigma=sigma),\n",
        "      nn.ReLU(),\n",
        "      NoisyLinear(hidden_size, hidden_size, sigma=sigma),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    # Dueling DQN\n",
        "    # We take the output from the previous hidden layers and create 2 parallel output layers\n",
        "    # One layer to represent the action rewards (advantage) and another for the state value (value)\n",
        "    # This algorithm helps to determine the states where taking no action is optimal\n",
        "    self.fc_adv = NoisyLinear(hidden_size, self.n_actions * self.atoms, sigma=sigma) # Distributional DQN atoms\n",
        "    self.fc_value = NoisyLinear(hidden_size, self.atoms, sigma=sigma) # Distributional DQN atoms\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.net(x)\n",
        "    adv = self.fc_adv(x).view(-1, self.n_actions, self.atoms)  # (B, A, N)\n",
        "    value = self.fc_value(x).view(-1, 1, self.atoms)  # (B, 1, N)\n",
        "    # Dueling DQN uses recomposes the V(s) and Q(s, a) components subtracting the mean advantage\n",
        "    q_logits = value + adv - adv.mean(dim=1, keepdim=True)  # (B, A, N)\n",
        "    # Distributional DQN - estimate Q probability distribution\n",
        "    q_probs = F.softmax(q_logits, dim=-1)  # (B, A, N)\n",
        "    return q_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnk0wSWj0hAz"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9a0b9cdnYtT"
      },
      "outputs": [],
      "source": [
        "def greedy(state, net, support):\n",
        "  \"\"\" Policy function for evaluating a given state and model\n",
        "\n",
        "  Args:\n",
        "      state (Tensor): The input state\n",
        "      net (nn.Module): The model\n",
        "      support (Tensor): The support tensor used to transform probabilities into q values\n",
        "\n",
        "  Returns:\n",
        "      int: The action chosen by the model for the given state\n",
        "  \"\"\"\n",
        "  state = torch.tensor(np.array([state])).to(device).to(torch.float32)\n",
        "  q_value_probs = net(state)  # (1, A, N) \n",
        "  q_values = (support * q_value_probs).sum(dim=-1)  # (1, A)\n",
        "  action = torch.argmax(q_values, dim=-1)  # (1, 1)\n",
        "  action = int(action.item())  # ()\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brJmKGkl0jge"
      },
      "source": [
        "#### Create the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvHMYqlZnYvj"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    \"\"\" A buffer for storing evaluated environment experiences\n",
        "\n",
        "    Args:\n",
        "        capacity (int): Max buffer size\n",
        "    \"\"\"\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "    self.priorities = deque(maxlen=capacity)\n",
        "    self.capacity = capacity\n",
        "    # Prioritized Experience Replay\n",
        "    # alpha: high -> low will choose batches with the most loss vs batches with the highest probability\n",
        "    # beta: low -> high corrects for the distribution bias of batches being chosen more frequently\n",
        "    self.alpha = 0.0  # null\n",
        "    self.beta = 1.0  # null\n",
        "    self.max_priority = 0.0\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "  \n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "    self.priorities.append(self.max_priority)\n",
        "  \n",
        "  def update(self, index, priority):\n",
        "    if priority > self.max_priority:\n",
        "      self.max_priority = priority\n",
        "    self.priorities[index] = priority\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    \"\"\" Return a sample from the buffer\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): Size of the sample\n",
        "\n",
        "    Returns:\n",
        "        list: The returned sample\n",
        "    \"\"\"\n",
        "    prios = np.array(self.priorities, dtype=np.float64) + 1e-4 # Stability constant.\n",
        "    prios = prios ** self.alpha\n",
        "    probs = prios / prios.sum()\n",
        "\n",
        "    weights = (self.__len__() * probs) ** -self.beta\n",
        "    weights = weights / weights.max()\n",
        "\n",
        "    idx = random.choices(range(self.__len__()), weights=probs, k=batch_size)\n",
        "    sample = [(i, weights[i], *self.buffer[i]) for i in idx]\n",
        "\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUQcRQ4xnYyI"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0yvDC9qF0oPr"
      },
      "source": [
        "#### Create and verify the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw-CWX4nap9c"
      },
      "outputs": [],
      "source": [
        "class MarketEnvironment:\n",
        "\n",
        "  def __init__(self, file, window):\n",
        "    self.data = list(pd.read_csv(file)['Adj Close'])\n",
        "    self.states = []\n",
        "\n",
        "    print(f'loaded {len(self.data)} data points')\n",
        "\n",
        "    # prepopulate the states to avoid needless loops per step\n",
        "    for idx, price in enumerate(self.data):\n",
        "      if idx >= window:\n",
        "        start = idx - window\n",
        "        block = self.data[start: idx + 1] \n",
        "        state = []\n",
        "        for i in range(window):\n",
        "          state.append(self._sigmoid(block[i+1] - block[i]))\n",
        "        self.states.append((price, state))\n",
        "    \n",
        "    print(f'generated {len(self.states)} states')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.states)\n",
        "        \n",
        "  def _sigmoid(self, x):\n",
        "    \"\"\"Performs sigmoid operation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if x < 0:\n",
        "            return 1 - 1 / (1 + math.exp(x))\n",
        "        return 1 / (1 + math.exp(-x))\n",
        "    except Exception as err:\n",
        "        print(\"Error in sigmoid: \" + err)\n",
        "\n",
        "  def __iter__(self):\n",
        "    for state in self.states:\n",
        "      yield state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2kQbrqV066I"
      },
      "outputs": [],
      "source": [
        "env = MarketEnvironment('data/GOOG_2010_SAMPLE.csv', 10)\n",
        "\n",
        "# verfy\n",
        "for i, state in enumerate(env):\n",
        "  print(f'state {i}: {state}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TradingAgent:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.num_actions = 3\n",
        "    self.reset()\n",
        "\n",
        "  def sample_action(self):\n",
        "    return random.randrange(3)\n",
        "\n",
        "  def take_action(self, action, price):\n",
        "    reward = 0\n",
        "\n",
        "    if action == 1:\n",
        "      self.inventory.append(price)\n",
        "    elif action == 2 and len(self.inventory) > 0:\n",
        "      buy_price = self.inventory.pop(0)\n",
        "      delta = price - buy_price\n",
        "      reward = delta\n",
        "      self.total_profit += reward\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def reset(self):\n",
        "    self.inventory = []\n",
        "    self.total_profit = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgXi6A4Z1p75"
      },
      "source": [
        "#### Create the Deep Q-Learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOmxUJ1vnY5d"
      },
      "outputs": [],
      "source": [
        "class DeepQLearning(LightningModule):\n",
        "\n",
        "  # Initialize.\n",
        "  def __init__(self, env_name, window=10, policy=greedy, capacity=100_000, \n",
        "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, optim=AdamW, \n",
        "               samples_per_epoch=10_000, sync_rate=10, sigma=0.5, a_start=0.5, \n",
        "               a_end=0.0, a_last_episode=100, b_start=0.4, b_end=1.0, b_last_episode=100, \n",
        "               n_steps=3, v_min=-10.0, v_max=10.0, atoms=51):\n",
        "    \"\"\" Rainbow Deep Q-Network \n",
        "\n",
        "    Args:\n",
        "        env_name (string): The market data file\n",
        "        window (int): The n period state size of each time t\n",
        "        policy (function, optional): Our policy function. Defaults to greedy.\n",
        "        capacity (int, optional): The max size of the replay buffer. Defaults to 100_000.\n",
        "        batch_size (int, optional): The size of each replay batch. Defaults to 256.\n",
        "        lr (float, optional): The network learning rate. Defaults to 1e-3.\n",
        "        hidden_size (int, optional): # of features in the hidden layers. Defaults to 128.\n",
        "        gamma (float, optional): Discount to future returns. Defaults to 0.99.\n",
        "        optim (class, optional): Gradient descent optimization class. Defaults to AdamW.\n",
        "        samples_per_epoch (_type_, optional): Buffer size needed for training to begin. Defaults to 10_000.\n",
        "        sync_rate (int, optional): The # of epochs for target net to sync. Defaults to 10.\n",
        "        sigma (float, optional): Noisey DQN std dev. Defaults to 0.5.\n",
        "        a_start (float, optional): Initial alpha for prioritized experience replay. Defaults to 0.5.\n",
        "        a_end (float, optional): Final alpha for prioritized experience replay. Defaults to 0.0.\n",
        "        a_last_episode (int, optional): Ending epoch for alpha. Defaults to 100.\n",
        "        b_start (float, optional): Initial beta for prioritized experience replay. Defaults to 0.4.\n",
        "        b_end (float, optional): Final alpha for prioritized experience replay. Defaults to 1.0.\n",
        "        b_last_episode (int, optional): Ending epoch for beta. Defaults to 100.\n",
        "        n_steps (int, optional): Number of steps for N-Step DQN. Defaults to 3.\n",
        "        v_min (float, optional): The minimum support value for Distributional DQN. Defaults to -10.0.\n",
        "        v_max (float, optional): The maximum support value for Distributional DQN. Defaults to 10.0.\n",
        "        atoms (int, optional): The number of atoms for Distributional DQN. Defaults to 51.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.support = torch.linspace(v_min, v_max, atoms, device=device)  # (N)\n",
        "    self.delta = (v_max - v_min) / (atoms - 1)\n",
        "\n",
        "    self.env = MarketEnvironment(env_name, window)\n",
        "    self.agent = TradingAgent()\n",
        "\n",
        "    self.q_net = DQN(hidden_size, window, self.agent.num_actions, atoms=atoms, sigma=sigma)\n",
        "\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "\n",
        "    self.policy = policy\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episode()\n",
        "    \n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None):\n",
        "    done = False\n",
        "    transitions = []\n",
        "    self.agent.reset()\n",
        "\n",
        "    for i, (price, state) in enumerate(self.env):\n",
        "      if policy:\n",
        "        action = policy(state, self.q_net, self.support)\n",
        "      else:\n",
        "        action = self.agent.sample_action()\n",
        "      reward = self.agent.take_action(action, price)\n",
        "      done = i == len(self.env) - 1\n",
        "      next_state = state if done else self.env.states[i + 1][1]\n",
        "      exp = (state, action, reward, done, next_state)\n",
        "      transitions.append(exp)\n",
        "\n",
        "    # N-Step DQN\n",
        "    # For each state t, caclulate the reward as the sum of discounted returns n -1 steps into the future\n",
        "    for i, (s, a, r, d, ns) in enumerate(transitions):\n",
        "      batch = transitions[i:i+self.hparams.n_steps]\n",
        "      ret = sum([t[2] * self.hparams.gamma**j for j, t in enumerate(batch)])\n",
        "      # The \"next state\" for each transition is the n future state used to calculate the training target\n",
        "      _, _, _, ld, ls = batch[-1]\n",
        "      self.buffer.append((s, a, ret, ld, ls))\n",
        "\n",
        "  # Forward.\n",
        "  def forward(self, x):\n",
        "    return self.q_net(x)\n",
        "\n",
        "  def custom_collate(self, data):\n",
        "    indices, weights, states, actions, returns, dones, next_states = zip(*data)\n",
        "\n",
        "    indices = torch.tensor(indices)\n",
        "    weights = torch.tensor(weights)\n",
        "    states = torch.tensor(states).to(torch.float32)\n",
        "    returns = torch.tensor(returns).to(torch.double)\n",
        "    dones = torch.tensor(dones)\n",
        "    next_states = torch.tensor(next_states).to(torch.float32)\n",
        "\n",
        "    return indices, weights, states, actions, returns, dones, next_states\n",
        "\n",
        "  # Configure optimizers.\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net_optimizer]\n",
        "\n",
        "  # Create dataloader.\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size,\n",
        "        num_workers=36,\n",
        "        collate_fn=self.custom_collate,\n",
        "        drop_last=True\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  # Training step\n",
        "  # Double DQN\n",
        "  # Target reward is the actual reward plus the discounted estimated reward in the next state\n",
        "  # We predict the discounted rewards from the target model, but choose the action predicted by the current model\n",
        "  # Then we update the target reward for the chosen action\n",
        "  # Finally we train the current model towards the target values by computing the loss between actual and target rewards\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    indices, weights, states, actions, returns, dones, next_states = batch\n",
        "    returns = returns.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "    batch_size = len(indices)\n",
        "\n",
        "    q_value_probs = self.q_net(states)  # (B, A, N)\n",
        "\n",
        "    action_value_probs = q_value_probs[range(batch_size), actions, :]  # (B, N)\n",
        "    log_action_value_probs = torch.log(action_value_probs + 1e-6)  # (B, N)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      next_q_value_probs = self.q_net(next_states)  # (B, A, N)\n",
        "      next_q_values = (next_q_value_probs * self.support).sum(dim=-1)  # (B, A)\n",
        "      next_actions = next_q_values.argmax(dim=-1)  # (B,)\n",
        "\n",
        "      next_q_value_probs = self.target_q_net(next_states)  # (B, A, N)\n",
        "      next_action_value_probs = next_q_value_probs[range(batch_size), next_actions, :]  # (B, N)\n",
        "\n",
        "    # Distributional DQN\n",
        "    # We must estimate m which is the target distrbution across n atoms from v_min to v_vmax for each action\n",
        "    m = torch.zeros(batch_size * self.hparams.atoms, device=device, dtype=torch.float64)  # (B * N)\n",
        "\n",
        "    # N-Step DQN discounts the future reward by gamma ** n, since rewards is the sum of 0 to n -1 discounted rewards\n",
        "    Tz = returns + ~dones * self.hparams.gamma**self.hparams.n_steps * self.support.unsqueeze(0)  # (B, N)\n",
        "\n",
        "    # Distributional DQN\n",
        "    # Account for the skew and the offset in Tz distribution which occurs from adding discounted rewards\n",
        "    Tz.clamp_(min=self.hparams.v_min, max=self.hparams.v_max)  # (B, N)\n",
        "    b = (Tz - self.hparams.v_min) / self.delta  # (B, N)\n",
        "    # Distribute fractional m values\n",
        "    l, u = b.floor().long(), b.ceil().long()  # (B, N)\n",
        "\n",
        "    offset = torch.arange(batch_size, device=device).view(-1, 1) * self.hparams.atoms  # (B, 1)\n",
        "\n",
        "    l_idx = (l + offset).flatten()  # (B * N)\n",
        "    u_idx = (u + offset).flatten()  # (B * N)\n",
        "    \n",
        "    upper_probs = (next_action_value_probs * (u - b)).flatten()  # (B * N)\n",
        "    lower_probs = (next_action_value_probs * (b - l)).flatten()  # (B * N)\n",
        "\n",
        "    m.index_add_(dim=0, index=l_idx, source=upper_probs)\n",
        "    m.index_add_(dim=0, index=u_idx, source=lower_probs)\n",
        "\n",
        "    m = m.reshape(batch_size, self.hparams.atoms)  # (B, N)\n",
        "\n",
        "    cross_entropies = - (m * log_action_value_probs).sum(dim=-1)  # (B,)\n",
        "\n",
        "    # Prioritized Experince Replay \n",
        "    # Update each state priority based on the loss\n",
        "    # Weight the calculated losses to account for distribution bias\n",
        "    for idx, e in zip(indices, cross_entropies):\n",
        "      self.buffer.update(idx, e.detach().item())\n",
        "\n",
        "    loss = (weights * cross_entropies).mean()\n",
        "\n",
        "    self.log('episode/Q-Error', loss)\n",
        "    return loss\n",
        "\n",
        "  # Training epoch end.\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "    # Prioritized Experience Replay\n",
        "    # Update the buffer alpha and beta parameters over time\n",
        "    alpha = max(\n",
        "        self.hparams.a_end,\n",
        "        self.hparams.a_start - self.current_epoch / self.hparams.a_last_episode\n",
        "    )\n",
        "    beta = min(\n",
        "        self.hparams.b_end,\n",
        "        self.hparams.b_start + self.current_epoch / self.hparams.b_last_episode\n",
        "    )\n",
        "    self.buffer.alpha = alpha\n",
        "    self.buffer.beta = beta\n",
        "\n",
        "    self.play_episode(policy=self.policy)\n",
        "    self.log('episode/Return', self.agent.total_profit)\n",
        "\n",
        "    if self.current_epoch % self.hparams.sync_rate == 0:\n",
        "      self.target_q_net.load_state_dict(self.q_net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mm9P0sX1wAA"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGQdpn0nY99"
      },
      "outputs": [],
      "source": [
        "!rm -r ./lightning_logs/\n",
        "!rm -r ./videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8GdIwla1wrW"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig8c_RM8nZLN"
      },
      "outputs": [],
      "source": [
        "algo = DeepQLearning(\n",
        "  'data/GOOG_2010_SAMPLE.csv',\n",
        "  lr=0.0001,\n",
        "  sigma=0.5,\n",
        "  hidden_size=512,\n",
        "  a_last_episode=2_000,\n",
        "  b_last_episode=2_000,\n",
        "  n_steps=8,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  strategy='dp',\n",
        "  accelerator='gpu',\n",
        "  devices=1,\n",
        "  max_epochs=2_400,\n",
        "  log_every_n_steps=1\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "9_distributional_dqn.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "trading-bot",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ec426baa9d129ec8706098ce32faa8d4810b5875946eeac7237d9121ac7f661a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
