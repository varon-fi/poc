{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxxWBZioi0N"
      },
      "source": [
        "Solve Trading Bot example using Rainbow DQN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJIVLLT1nYMl"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install \\\n",
        "  pytorch-lightning==1.6.0 \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lex/Repos/varon-fi/.conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device cuda:0 num_gpus 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lex/Repos/varon-fi/.conda/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
            "/home/lex/Repos/varon-fi/.conda/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  np.bool8: (False, True),\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import TransformObservation, NormalizeObservation, \\\n",
        "  NormalizeReward, RecordVideo, RecordEpisodeStatistics, AtariPreprocessing\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "print(f'device {device} num_gpus {num_gpus}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLH52SgC0RRI"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DZO9u9X1CTtf"
      },
      "outputs": [],
      "source": [
        "import math \n",
        "from torch.nn.init import kaiming_uniform_, zeros_\n",
        "\n",
        "class NoisyLinear(nn.Module):\n",
        "\n",
        "  def __init__(self, in_features, out_features, sigma):\n",
        "    super(NoisyLinear, self).__init__()\n",
        "    self.w_mu = nn.Parameter(torch.empty((out_features, in_features)))\n",
        "    self.w_sigma = nn.Parameter(torch.empty((out_features, in_features)))\n",
        "    self.b_mu = nn.Parameter(torch.empty((out_features)))\n",
        "    self.b_sigma = nn.Parameter(torch.empty((out_features)))\n",
        "\n",
        "    kaiming_uniform_(self.w_mu, a=math.sqrt(5))\n",
        "    kaiming_uniform_(self.w_sigma, a=math.sqrt(5))\n",
        "    zeros_(self.b_mu)\n",
        "    zeros_(self.b_sigma)\n",
        "    \n",
        "  def forward(self, x, sigma=0.5):\n",
        "    if self.training:\n",
        "      w_noise = torch.normal(0, sigma, size=self.w_mu.size()).to(device)\n",
        "      b_noise = torch.normal(0, sigma, size=self.b_mu.size()).to(device)\n",
        "      return F.linear(x, self.w_mu + self.w_sigma * w_noise, self.b_mu + self.b_sigma * b_noise)\n",
        "    else:\n",
        "      return F.linear(x, self.W_mu, self.b_mu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x6gm8-15nYq7"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_shape, n_actions, atoms=51, sigma=0.5):\n",
        "    super().__init__()\n",
        "    self.atoms = atoms\n",
        "    self.n_actions = n_actions\n",
        "    \n",
        "    self.conv = nn.Sequential(\n",
        "      nn.Conv2d(obs_shape[0], 64, kernel_size=3),\n",
        "      nn.MaxPool2d(kernel_size=4),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size=3),\n",
        "      nn.MaxPool2d(kernel_size=4),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "    conv_out_size = self._get_conv_out(obs_shape)\n",
        "    self.head = nn.Sequential(\n",
        "      NoisyLinear(conv_out_size, hidden_size, sigma=sigma),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.fc_adv = NoisyLinear(hidden_size, self.n_actions * self.atoms, sigma=sigma) \n",
        "    self.fc_value = NoisyLinear(hidden_size, self.atoms, sigma=sigma)\n",
        "\n",
        "  def _get_conv_out(self, shape):\n",
        "    conv_out = self.conv(torch.zeros(1, *shape))\n",
        "    return int(np.prod(conv_out.size()))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv(x.float()).view(x.size()[0], -1)\n",
        "    x = self.head(x)\n",
        "    adv = self.fc_adv(x).view(-1, self.n_actions, self.atoms)  # (B, A, N)\n",
        "    value = self.fc_value(x).view(-1, 1, self.atoms)  # (B, 1, N)\n",
        "    q_logits = value + adv - adv.mean(dim=1, keepdim=True)  # (B, A, N)\n",
        "    q_probs = F.softmax(q_logits, dim=-1)  # (B, A, N)\n",
        "    return q_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnk0wSWj0hAz"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o9a0b9cdnYtT"
      },
      "outputs": [],
      "source": [
        "def greedy(state, net, support):\n",
        "  state = torch.tensor(np.array([state])).to(device)\n",
        "  q_value_probs = net(state)  # (1, A, N) \n",
        "  q_values = (support * q_value_probs).sum(dim=-1)  # (1, A)\n",
        "  action = torch.argmax(q_values, dim=-1)  # (1, 1)\n",
        "  action = int(action.item())  # ()\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brJmKGkl0jge"
      },
      "source": [
        "#### Create the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MvHMYqlZnYvj"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "    self.priorities = deque(maxlen=capacity)\n",
        "    self.capacity = capacity\n",
        "    self.alpha = 0.0  # anneal.\n",
        "    self.beta = 1.0  # anneal.\n",
        "    self.max_priority = 0.0\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "  \n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "    self.priorities.append(self.max_priority)\n",
        "  \n",
        "  def update(self, index, priority):\n",
        "    if priority > self.max_priority:\n",
        "      self.max_priority = priority\n",
        "    self.priorities[index] = priority\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    prios = np.array(self.priorities, dtype=np.float64) + 1e-4 # Stability constant.\n",
        "    prios = prios ** self.alpha\n",
        "    probs = prios / prios.sum()\n",
        "\n",
        "    weights = (self.__len__() * probs) ** -self.beta\n",
        "    weights = weights / weights.max()\n",
        "\n",
        "    idx = random.choices(range(self.__len__()), weights=probs, k=batch_size)\n",
        "    sample = [(i, weights[i], *self.buffer[i]) for i in idx]\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iUQcRQ4xnYyI"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zw-CWX4nap9c"
      },
      "outputs": [],
      "source": [
        "class TradingBotEnvironment:\n",
        "\n",
        "  def __init__(self, file, state_size):\n",
        "    self.file = file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgXi6A4Z1p75"
      },
      "source": [
        "#### Create the Deep Q-Learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOmxUJ1vnY5d"
      },
      "outputs": [],
      "source": [
        "class DeepQLearning(LightningModule):\n",
        "\n",
        "  # Initialize.\n",
        "  def __init__(self, env_name, policy=greedy, capacity=100_000, \n",
        "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, \n",
        "               loss_fn=F.smooth_l1_loss, optim=AdamW, samples_per_epoch=10_000, \n",
        "               sync_rate=10, sigma=0.5, a_start=0.5, a_end=0.0, a_last_episode=100, \n",
        "               b_start=0.4, b_end=1.0, b_last_episode=100, n_steps=3, \n",
        "               v_min=-10.0, v_max=10.0, atoms=51):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.support = torch.linspace(v_min, v_max, atoms, device=device)  # (N)\n",
        "    self.delta = (v_max - v_min) / (atoms - 1)\n",
        "\n",
        "    self.env = TradingBotEnvironment(env_name)\n",
        "\n",
        "    obs_size = self.env.observation_space.shape\n",
        "    n_actions = self.env.action_space.n\n",
        "\n",
        "    self.q_net = DQN(hidden_size, obs_size, n_actions, atoms=atoms, sigma=sigma)\n",
        "\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "\n",
        "    self.policy = policy\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episode()\n",
        "    \n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None):\n",
        "    state = self.env.reset()\n",
        "    done = False\n",
        "    transitions = []\n",
        "\n",
        "    while not done:\n",
        "      if policy:\n",
        "        action = policy(state, self.q_net, self.support)\n",
        "      else:\n",
        "        action = self.env.action_space.sample()\n",
        "      \n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      exp = (state, action, reward, done, next_state)\n",
        "      transitions.append(exp)\n",
        "      state = next_state\n",
        "\n",
        "    for i, (s, a, r, d, ns) in enumerate(transitions):\n",
        "      batch = transitions[i:i+self.hparams.n_steps]\n",
        "      ret = sum([t[2] * self.hparams.gamma**j for j, t in enumerate(batch)])\n",
        "      _, _, _, ld, ls = batch[-1]\n",
        "      self.buffer.append((s, a, ret, ld, ls))\n",
        "\n",
        "  # Forward.\n",
        "  def forward(self, x):\n",
        "    return self.q_net(x)\n",
        "\n",
        "  # Configure optimizers.\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net_optimizer]\n",
        "\n",
        "  # Create dataloader.\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size,\n",
        "        num_workers=36\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  # Training step.\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    indices, weights, states, actions, returns, dones, next_states = batch\n",
        "    returns = returns.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "    batch_size = len(indices)\n",
        "\n",
        "    q_value_probs = self.q_net(states)  # (B, A, N)\n",
        "\n",
        "    action_value_probs = q_value_probs[range(batch_size), actions, :]  # (B, N)\n",
        "    log_action_value_probs = torch.log(action_value_probs + 1e-6)  # (B, N)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      next_q_value_probs = self.q_net(next_states)  # (B, A, N)\n",
        "      next_q_values = (next_q_value_probs * self.support).sum(dim=-1)  # (B, A)\n",
        "      next_actions = next_q_values.argmax(dim=-1)  # (B,)\n",
        "\n",
        "      next_q_value_probs = self.target_q_net(next_states)  # (B, A, N)\n",
        "      next_action_value_probs = next_q_value_probs[range(batch_size), next_actions, :]  # (B, N)\n",
        "\n",
        "    m = torch.zeros(batch_size * self.hparams.atoms, device=device, dtype=torch.float64)  # (B * N)\n",
        "\n",
        "    Tz = returns + ~dones * self.hparams.gamma**self.hparams.n_steps * self.support.unsqueeze(0)  # (B, N)\n",
        "\n",
        "    Tz.clamp_(min=self.hparams.v_min, max=self.hparams.v_max)  # (B, N)\n",
        "    b = (Tz - self.hparams.v_min) / self.delta  # (B, N)\n",
        "    l, u = b.floor().long(), b.ceil().long()  # (B, N)\n",
        "\n",
        "    offset = torch.arange(batch_size, device=device).view(-1, 1) * self.hparams.atoms  # (B, 1)\n",
        "\n",
        "    l_idx = (l + offset).flatten()  # (B * N)\n",
        "    u_idx = (u + offset).flatten()  # (B * N)\n",
        "    \n",
        "    upper_probs = (next_action_value_probs * (u - b)).flatten()  # (B * N)\n",
        "    lower_probs = (next_action_value_probs * (b - l)).flatten()  # (B * N)\n",
        "\n",
        "    m.index_add_(dim=0, index=l_idx, source=upper_probs)\n",
        "    m.index_add_(dim=0, index=u_idx, source=lower_probs)\n",
        "\n",
        "    m = m.reshape(batch_size, self.hparams.atoms)  # (B, N)\n",
        "\n",
        "    cross_entropies = - (m * log_action_value_probs).sum(dim=-1)  # (B,)\n",
        "\n",
        "    for idx, e in zip(indices, cross_entropies):\n",
        "      self.buffer.update(idx, e.detach().item())\n",
        "\n",
        "    loss = (weights * cross_entropies).mean()\n",
        "\n",
        "    self.log('episode/Q-Error', loss)\n",
        "    return loss\n",
        "\n",
        "  # Training epoch end.\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "    alpha = max(\n",
        "        self.hparams.a_end,\n",
        "        self.hparams.a_start - self.current_epoch / self.hparams.a_last_episode\n",
        "    )\n",
        "    beta = min(\n",
        "        self.hparams.b_end,\n",
        "        self.hparams.b_start + self.current_epoch / self.hparams.b_last_episode\n",
        "    )\n",
        "    self.buffer.alpha = alpha\n",
        "    self.buffer.beta = beta\n",
        "\n",
        "    self.play_episode(policy=self.policy)\n",
        "    self.log('episode/Return', self.env.return_queue[-1])\n",
        "\n",
        "    if self.current_epoch % self.hparams.sync_rate == 0:\n",
        "      self.target_q_net.load_state_dict(self.q_net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mm9P0sX1wAA"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGQdpn0nY99"
      },
      "outputs": [],
      "source": [
        "!rm -r ./lightning_logs/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8GdIwla1wrW"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig8c_RM8nZLN"
      },
      "outputs": [],
      "source": [
        "algo = DeepQLearning(\n",
        "  'QbertNoFrameskip-v4',\n",
        "  lr=0.0001,\n",
        "  sigma=0.5,\n",
        "  hidden_size=512,\n",
        "  a_last_episode=2_000,\n",
        "  b_last_episode=2_000,\n",
        "  n_steps=8,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  strategy='dp',\n",
        "  accelerator='gpu',\n",
        "  devices=1,\n",
        "  max_epochs=2_400,\n",
        "  log_every_n_steps=1\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "9_distributional_dqn.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "0890f3a9f74b1f5acae0ac443fa3a574174a5f8e1d0754c49718f9ebe56d6827"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
